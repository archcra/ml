{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是在http://58.241.217.181:15111/notebooks/work/ml/gnt-a/notebooks/code/customized-model-A.ipynb#\n",
    "的基础上，参照：\n",
    "https://www.tensorflow.org/versions/master/tutorials/layers\n",
    "来制作CNN，用于识别GNT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing path:  ../data/train_/00037\n",
      "self.images:  float64\n",
      "self.images:  float32\n",
      "Train data loaded ...\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../dfs/checkpoint/customized_model-c', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "Begin to train ...\n",
      "shape of input_layer:  Tensor(\"Reshape:0\", shape=(100, 64, 64, 1), dtype=float32)\n",
      "shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(100, 64, 64, 32), dtype=float32)\n",
      "shape of conv2:  Tensor(\"conv2d_2/Relu:0\", shape=(100, 32, 32, 64), dtype=float32) ; and shape of pool2 is:  Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(100, 16, 16, 64), dtype=float32)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ../dfs/checkpoint/customized_model-c/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.62224, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.934288\n",
      "INFO:tensorflow:loss = 3.60683, step = 101 (107.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.936267\n",
      "INFO:tensorflow:loss = 3.62841, step = 201 (106.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.941368\n",
      "INFO:tensorflow:loss = 3.59767, step = 301 (106.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.93622\n",
      "INFO:tensorflow:loss = 3.58382, step = 401 (106.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.925153\n",
      "INFO:tensorflow:loss = 3.60644, step = 501 (108.090 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 561 into ../dfs/checkpoint/customized_model-c/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.918388\n",
      "INFO:tensorflow:loss = 3.60125, step = 601 (108.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.987424\n",
      "INFO:tensorflow:loss = 3.62245, step = 701 (101.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.09401\n",
      "INFO:tensorflow:loss = 3.58907, step = 801 (91.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.10442\n",
      "INFO:tensorflow:loss = 3.59079, step = 901 (90.547 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ../dfs/checkpoint/customized_model-c/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.59949.\n",
      "Train done ...\n",
      "Now processing path:  ../data/test_/00037\n",
      "self.images:  float64\n",
      "self.images:  float32\n",
      "Test data loaded ...\n",
      "shape of input_layer:  Tensor(\"Reshape:0\", shape=(?, 64, 64, 1), dtype=float32)\n",
      "shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "shape of conv2:  Tensor(\"conv2d_2/Relu:0\", shape=(?, 32, 32, 64), dtype=float32) ; and shape of pool2 is:  Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-26-06:35:15\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-c/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-26-06:35:21\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.106335, global_step = 1000, loss = 3.58817\n",
      "\n",
      "Test Accuracy: 10.633484%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 64, 64, 1])\n",
    "  print ('shape of input_layer: ', input_layer)\n",
    "  # with batch_size =100, shape should be: [100, 28, 28, 1]\n",
    "  # shape of input_layer:  Tensor(\"Reshape:0\", shape=(100, 64, 64, 1), dtype=float32)\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print ('shape of conv1: ', conv1)\n",
    "  # shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(100, 64, 64, 32), dtype=float32)\n",
    "    \n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "  print ('shape of conv2: ', conv2, '; and shape of pool2 is: ', pool2)\n",
    "\n",
    "  # Dense Layer\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 16 * 16 * 64])\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=CHARSET_SIZE)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set FLAGS.charset_size to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        print ('self.images: ', self.images[0].dtype)\n",
    "        self.images = np.float32(self.images)\n",
    "        print ('self.images: ', self.images[0].dtype)\n",
    "\n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "    \n",
    "train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "print ('Train data loaded ...')\n",
    "\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "\n",
    "\n",
    "# Build 2 hidden layer DNN with ...\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn,\n",
    "   model_dir='../dfs/checkpoint/customized_model-c')\n",
    "\n",
    "    \n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(train_data)[0]},\n",
    "    y=input(train_data)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "print ('Begin to train ...')\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=1000)\n",
    "print ('Train done ...')\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "print ('Test data loaded ...')\n",
    "\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义CNN雏形完成：\n",
    "INFO:tensorflow:Starting evaluation at 2018-04-26-06:35:15\n",
    "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-c/model.ckpt-1000\n",
    "INFO:tensorflow:Finished evaluation at 2018-04-26-06:35:21\n",
    "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.106335, global_step = 1000, loss = 3.58817\n",
    "\n",
    "Test Accuracy: 10.633484%\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
