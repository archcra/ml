{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面根据customized c 模型，做一些调整。以期望达到更快的收敛。\n",
    "\n",
    "1. 加两个卷积层\n",
    "2. 加3个dense层\n",
    "3. 将卷积的核，从5*5 改为3*3\n",
    "\n",
    "\n",
    "## error\n",
    "ValueError: Dimension size must be evenly divisible by 16384 but is 102400 for 'Reshape_1' (op: 'Reshape') with input shapes: [100,4,4,64], [2] and with input tensors computed as partial shapes: input[1] = [?,16384].\n",
    "\n",
    "\n",
    "shape of conv4:  Tensor(\"conv2d_4/Relu:0\", shape=(100, 8, 8, 64), dtype=float32) ; and shape of pool4 is:  Tensor(\"max_pooling2d_4/MaxPool:0\", shape=(100, 4, 4, 64), dtype=float32)\n",
    "\n",
    "上一层，与下一层的参数关系？\n",
    "\n",
    "```\n",
    "# Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv4 = tf.layers.conv2d(\n",
    "      inputs=pool3,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "  # Dense Layer\n",
    "  print ('shape of conv4: ', conv4, '; and shape of pool4 is: ', pool4)\n",
    "\n",
    "  pool4_flat = tf.reshape(pool4, [-1, 16 * 16 * 64])\n",
    "  dense1 = tf.layers.dense(inputs=pool4_flat, units=1024, activation=tf.nn.relu)\n",
    "```\n",
    "\n",
    "将这个：  pool4_flat = tf.reshape(pool4, [-1, 16 * 16 * 64])\n",
    "改为和上一层的shape一样：\n",
    "  pool4_flat = tf.reshape(pool4, [-1, 4 * 4 * 64])\n",
    "就不再报错了。\n",
    "\n",
    "\n",
    "INFO:tensorflow:Starting evaluation at 2018-04-27-11:18:50\n",
    "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-d/model.ckpt-100\n",
    "INFO:tensorflow:Finished evaluation at 2018-04-27-11:18:57\n",
    "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.0271493, global_step = 100, loss = 3.61113\n",
    "\n",
    "Test Accuracy: 2.714932%\n",
    "\n",
    "INFO:tensorflow:Starting evaluation at 2018-04-27-11:41:36\n",
    "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-d/model.ckpt-1100\n",
    "INFO:tensorflow:Finished evaluation at 2018-04-27-11:41:44\n",
    "INFO:tensorflow:Saving dict for global step 1100: accuracy = 0.0271493, global_step = 1100, loss = 3.61091\n",
    "\n",
    "Test Accuracy: 2.714932%\n",
    "\n",
    "\n",
    "INFO:tensorflow:Starting evaluation at 2018-04-27-12:25:38\n",
    "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-d/model.ckpt-2100\n",
    "INFO:tensorflow:Finished evaluation at 2018-04-27-12:25:45\n",
    "INFO:tensorflow:Saving dict for global step 2100: accuracy = 0.0285068, global_step = 2100, loss = 3.61116\n",
    "\n",
    "Test Accuracy: 2.850679%\n",
    "\n",
    "下面的模型虽然不出错，但是感觉有问题：\n",
    "INFO:tensorflow:Starting evaluation at 2018-04-28-02:09:40\n",
    "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-d/model.ckpt-5100\n",
    "INFO:tensorflow:Finished evaluation at 2018-04-28-02:09:48\n",
    "INFO:tensorflow:Saving dict for global step 5100: accuracy = 0.0380091, global_step = 5100, loss = 3.61053\n",
    "\n",
    "Test Accuracy: 3.800905%\n",
    "\n",
    "\n",
    "感觉这个层数组合，是有说法的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing path:  ../data/train_/00037\n",
      "INFO:tensorflow:global_step/sec: 0.831184\n",
      "INFO:tensorflow:loss = 3.61759, step = 2201 (120.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.842564\n",
      "INFO:tensorflow:loss = 3.61024, step = 2301 (118.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.845224\n",
      "INFO:tensorflow:loss = 3.60715, step = 2401 (118.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.846627\n",
      "INFO:tensorflow:loss = 3.61285, step = 2501 (118.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.851396\n",
      "INFO:tensorflow:loss = 3.61231, step = 2601 (117.454 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2607 into ../dfs/checkpoint/customized_model-d/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.849203\n",
      "INFO:tensorflow:loss = 3.6086, step = 2701 (117.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.828508\n",
      "INFO:tensorflow:loss = 3.60935, step = 2801 (120.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.845095\n",
      "INFO:tensorflow:loss = 3.61098, step = 2901 (118.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.858014\n",
      "INFO:tensorflow:loss = 3.6061, step = 3001 (116.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.836523\n",
      "INFO:tensorflow:loss = 3.61113, step = 3101 (119.544 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3113 into ../dfs/checkpoint/customized_model-d/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.81117\n",
      "INFO:tensorflow:loss = 3.60924, step = 3201 (123.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.836075\n",
      "INFO:tensorflow:loss = 3.60998, step = 3301 (119.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.842283\n",
      "INFO:tensorflow:loss = 3.60906, step = 3401 (118.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.821476\n",
      "INFO:tensorflow:loss = 3.61333, step = 3501 (121.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.840364\n",
      "INFO:tensorflow:loss = 3.61139, step = 3601 (118.996 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3612 into ../dfs/checkpoint/customized_model-d/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.844608\n",
      "INFO:tensorflow:loss = 3.61377, step = 3701 (118.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.832183\n",
      "INFO:tensorflow:loss = 3.61396, step = 3801 (120.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.840232\n",
      "INFO:tensorflow:loss = 3.60973, step = 3901 (119.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.845477\n",
      "INFO:tensorflow:loss = 3.61303, step = 4001 (118.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.843915\n",
      "INFO:tensorflow:loss = 3.60732, step = 4101 (118.493 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4118 into ../dfs/checkpoint/customized_model-d/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.832248\n",
      "INFO:tensorflow:loss = 3.61244, step = 4201 (120.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.841686\n",
      "INFO:tensorflow:loss = 3.61173, step = 4301 (118.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.84636\n",
      "INFO:tensorflow:loss = 3.60637, step = 4401 (118.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.842182\n",
      "INFO:tensorflow:loss = 3.61402, step = 4501 (118.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.860961\n",
      "INFO:tensorflow:loss = 3.61013, step = 4601 (116.146 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4625 into ../dfs/checkpoint/customized_model-d/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.841319\n",
      "INFO:tensorflow:loss = 3.61109, step = 4701 (118.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.830866\n",
      "INFO:tensorflow:loss = 3.60981, step = 4801 (120.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.823466\n",
      "INFO:tensorflow:loss = 3.60851, step = 4901 (121.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.832273\n",
      "INFO:tensorflow:loss = 3.61115, step = 5001 (120.150 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5100 into ../dfs/checkpoint/customized_model-d/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.61269.\n",
      "Train done ...\n",
      "Now processing path:  ../data/test_/00037\n",
      "self.images:  float64\n",
      "self.images:  float32\n",
      "Test data loaded ...\n",
      "shape of input_layer:  Tensor(\"Reshape:0\", shape=(?, 64, 64, 1), dtype=float32)\n",
      "shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "shape of conv2:  Tensor(\"conv2d_2/Relu:0\", shape=(?, 32, 32, 64), dtype=float32) ; and shape of pool2 is:  Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "shape of conv4:  Tensor(\"conv2d_4/Relu:0\", shape=(?, 8, 8, 64), dtype=float32) ; and shape of pool4 is:  Tensor(\"max_pooling2d_4/MaxPool:0\", shape=(?, 4, 4, 64), dtype=float32)\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-27-13:26:43\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-d/model.ckpt-5100\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-27-13:26:50\n",
      "INFO:tensorflow:Saving dict for global step 5100: accuracy = 0.0380091, global_step = 5100, loss = 3.6107\n",
      "\n",
      "Test Accuracy: 3.800905%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 64, 64, 1])\n",
    "  print ('shape of input_layer: ', input_layer)\n",
    "  # with batch_size =100, shape should be: [100, 28, 28, 1]\n",
    "  # shape of input_layer:  Tensor(\"Reshape:0\", shape=(100, 64, 64, 1), dtype=float32)\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print ('shape of conv1: ', conv1)\n",
    "  # shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(100, 64, 64, 32), dtype=float32)\n",
    "    \n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "  print ('shape of conv2: ', conv2, '; and shape of pool2 is: ', pool2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv4 = tf.layers.conv2d(\n",
    "      inputs=pool3,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "  # Dense Layer\n",
    "  print ('shape of conv4: ', conv4, '; and shape of pool4 is: ', pool4)\n",
    "\n",
    "  pool4_flat = tf.reshape(pool4, [-1, 4 * 4 * 64])\n",
    "  dense1 = tf.layers.dense(inputs=pool4_flat, units=1024, activation=tf.nn.relu)\n",
    "  dense2 = tf.layers.dense(inputs=dense1, units=1024, activation=tf.nn.relu)\n",
    "  dense3 = tf.layers.dense(inputs=dense2, units=1024, activation=tf.nn.relu)\n",
    "  dense4 = tf.layers.dense(inputs=dense3, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense4, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=CHARSET_SIZE)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set FLAGS.charset_size to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        print ('self.images: ', self.images[0].dtype)\n",
    "        self.images = np.float32(self.images)\n",
    "        print ('self.images: ', self.images[0].dtype)\n",
    "\n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "    \n",
    "train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "print ('Train data loaded ...')\n",
    "\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "\n",
    "\n",
    "# Build CNN with customized function ...\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn,\n",
    "   model_dir='../dfs/checkpoint/customized_model-d')\n",
    "\n",
    "    \n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(train_data)[0]},\n",
    "    y=input(train_data)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "print ('Begin to train ...')\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=3000)\n",
    "print ('Train done ...')\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "print ('Test data loaded ...')\n",
    "\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是直接测试版：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../dfs/checkpoint/customized_model-d', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "Now processing path:  ../data/test_/00037\n",
      "shape of input_layer:  Tensor(\"Reshape:0\", shape=(?, 64, 64, 1), dtype=float32)\n",
      "shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "shape of conv2:  Tensor(\"conv2d_2/Relu:0\", shape=(?, 32, 32, 64), dtype=float32) ; and shape of pool2 is:  Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "shape of conv4:  Tensor(\"conv2d_4/Relu:0\", shape=(?, 8, 8, 64), dtype=float32) ; and shape of pool4 is:  Tensor(\"max_pooling2d_4/MaxPool:0\", shape=(?, 4, 4, 64), dtype=float32)\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-28-02:09:40\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/customized_model-d/model.ckpt-5100\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-28-02:09:48\n",
      "INFO:tensorflow:Saving dict for global step 5100: accuracy = 0.0380091, global_step = 5100, loss = 3.61053\n",
      "\n",
      "Test Accuracy: 3.800905%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from: https://www.kaggle.com/jeffcarp/example-save-and-load-a-tensorflow-model\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.estimators import run_config\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 64, 64, 1])\n",
    "  print ('shape of input_layer: ', input_layer)\n",
    "  # with batch_size =100, shape should be: [100, 28, 28, 1]\n",
    "  # shape of input_layer:  Tensor(\"Reshape:0\", shape=(100, 64, 64, 1), dtype=float32)\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print ('shape of conv1: ', conv1)\n",
    "  # shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(100, 64, 64, 32), dtype=float32)\n",
    "    \n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "  print ('shape of conv2: ', conv2, '; and shape of pool2 is: ', pool2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv4 = tf.layers.conv2d(\n",
    "      inputs=pool3,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "  # Dense Layer\n",
    "  print ('shape of conv4: ', conv4, '; and shape of pool4 is: ', pool4)\n",
    "\n",
    "  pool4_flat = tf.reshape(pool4, [-1, 4 * 4 * 64])\n",
    "  dense1 = tf.layers.dense(inputs=pool4_flat, units=1024, activation=tf.nn.relu)\n",
    "  dense2 = tf.layers.dense(inputs=dense1, units=1024, activation=tf.nn.relu)\n",
    "  dense3 = tf.layers.dense(inputs=dense2, units=1024, activation=tf.nn.relu)\n",
    "  dense4 = tf.layers.dense(inputs=dense3, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense4, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=CHARSET_SIZE)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set CHARSET_SIZE to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        self.images = np.float32(self.images)\n",
    "        # or else: TypeError: Value passed to parameter 'input' has DataType float64 not in list of allowed values: float16, float32\n",
    "\n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "# Build CNN with customized function ...\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn,\n",
    "   model_dir='../dfs/checkpoint/customized_model-d')\n",
    "\n",
    "\n",
    "\n",
    "# MODEL_DIR = \"../dfs/checkpoint/customized_model-c\"\n",
    "# model_from_checkpoint = make_estimator(MODEL_DIR)\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
