{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一个自定义model，训练精度极低，效率极低。主要是没有按照规律设计，不了解原理而硬造出来的模型。\n",
    "\n",
    "上一个模型是这样的：\n",
    "\n",
    "input_layer + conv1 + pool1 + conv2 + pool2 + conv3 + pool3 + conv4 + pool4 + dense1 + dense2 + dense3 + dense4 + logits\n",
    "\n",
    "这个层倒是很多，hidden layers 有12层！\n",
    "\n",
    "不过，下面是两处关于CNN设计的建议：\n",
    "\n",
    "https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n",
    "\n",
    "这里面，有cnn间的shape计算，以及层设计的建议：\n",
    "\n",
    "\n",
    "![Typical architecture of CNN](https://cdn-images-1.medium.com/max/1600/1*2SWb6CmxzbPZijmevFbe-g.jpeg)\n",
    "\n",
    "上面的图可能显示不出来，它是这样建议的：\n",
    "\n",
    "conv + conv + pool + conv + conv + pool + conv + conv + pool + ...\n",
    "\n",
    "这是另一个地方的建议：\n",
    "\n",
    "INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-6f5b7f3f5bf3>, line 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6f5b7f3f5bf3>\"\u001b[0;36m, line \u001b[0;32m128\u001b[0m\n\u001b[0;31m    model_dir='../dfs/checkpoint/customized_model-e')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 64, 64, 1])\n",
    "  print ('shape of input_layer: ', input_layer)\n",
    "  # with batch_size =100, shape should be: [100, 28, 28, 1]\n",
    "  # shape of input_layer:  Tensor(\"Reshape:0\", shape=(100, 64, 64, 1), dtype=float32)\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print ('shape of conv1: ', conv1)\n",
    "  # shape of conv1:  Tensor(\"conv2d/Relu:0\", shape=(100, 64, 64, 32), dtype=float32)\n",
    "    \n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "  print ('shape of pool1: ', pool1)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "  print ('shape of conv2: ', conv2, '; and shape of pool2 is: ', pool2)\n",
    "  # shape of conv2:  Tensor(\"conv2d_2/Relu:0\", shape=(100, 32, 32, 64), dtype=float32) ; and shape of pool2 is:  Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(100, 16, 16, 64), dtype=float32)\n",
    "  \n",
    "  # Dense Layers\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 16 * 16 * 64])\n",
    "  dense1 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "  dense2 = tf.layers.dense(inputs=dense1, units=1024, activation=tf.nn.relu)\n",
    "  dense3 = tf.layers.dense(inputs=dense2, units=1024, activation=tf.nn.relu)\n",
    "  dense4 = tf.layers.dense(inputs=dense3, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense4, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=CHARSET_SIZE)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set FLAGS.charset_size to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        print ('self.images: ', self.images[0].dtype)\n",
    "        self.images = np.float32(self.images)\n",
    "        print ('self.images: ', self.images[0].dtype)\n",
    "\n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "    \n",
    "train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "print ('Train data loaded ...')\n",
    "\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "\n",
    "# Build CNN with customized function ...\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn ,\n",
    "    model_dir='../dfs/checkpoint/customized_model-e')\n",
    "\n",
    "    \n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(train_data)[0]},\n",
    "    y=input(train_data)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "print ('Begin to train ...')\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=20000)\n",
    "print ('Train done ...')\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "print ('Test data loaded ...')\n",
    "\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO:tensorflow:Starting evaluation at 2018-04-30-10:32:08\n",
    "INFO:tensorflow:Restoring parameters from /tmp/tmp4bp3lczu/model.ckpt-10000\n",
    "INFO:tensorflow:Finished evaluation at 2018-04-30-10:32:18\n",
    "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.0733032, global_step = 10000, loss = 3.59994\n",
    "\n",
    "Test Accuracy: 7.330317%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
