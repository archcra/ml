{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的A和B，分别是对于学习数据做了一些熟悉。\n",
    "下面，我们看我们基于的学习代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/mnist_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/mnist_model/model.ckpt-1\n",
      "INFO:tensorflow:Saving checkpoints for 2 into ./tmp/mnist_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 117.992, step = 2\n",
      "INFO:tensorflow:global_step/sec: 191.025\n",
      "INFO:tensorflow:loss = 81.7786, step = 102 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.654\n",
      "INFO:tensorflow:loss = 48.4067, step = 202 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.35\n",
      "INFO:tensorflow:loss = 38.9902, step = 302 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.767\n",
      "INFO:tensorflow:loss = 28.77, step = 402 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.733\n",
      "INFO:tensorflow:loss = 38.4787, step = 502 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.629\n",
      "INFO:tensorflow:loss = 27.9483, step = 602 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.368\n",
      "INFO:tensorflow:loss = 20.3893, step = 702 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.355\n",
      "INFO:tensorflow:loss = 22.782, step = 802 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.343\n",
      "INFO:tensorflow:loss = 21.8841, step = 902 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.542\n",
      "INFO:tensorflow:loss = 20.6206, step = 1002 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.71\n",
      "INFO:tensorflow:loss = 21.8951, step = 1102 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.009\n",
      "INFO:tensorflow:loss = 14.9775, step = 1202 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.946\n",
      "INFO:tensorflow:loss = 18.3848, step = 1302 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.471\n",
      "INFO:tensorflow:loss = 15.3948, step = 1402 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.042\n",
      "INFO:tensorflow:loss = 16.7643, step = 1502 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.277\n",
      "INFO:tensorflow:loss = 18.473, step = 1602 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.133\n",
      "INFO:tensorflow:loss = 16.08, step = 1702 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.242\n",
      "INFO:tensorflow:loss = 17.4515, step = 1802 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.214\n",
      "INFO:tensorflow:loss = 16.9019, step = 1902 (0.482 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into ./tmp/mnist_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.58294.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-18-08:23:31\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/mnist_model/model.ckpt-2001\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-18-08:23:31\n",
      "INFO:tensorflow:Saving dict for global step 2001: accuracy = 0.9334, average_loss = 0.241994, global_step = 2001, loss = 30.6322\n",
      "\n",
      "Test Accuracy: 93.339998%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels.astype(np.int32)\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
    "\n",
    "# Build 2 layer DNN classifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[256, 32],\n",
    "    optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "    n_classes=10,\n",
    "    dropout=0.1,\n",
    "    model_dir=\"./tmp/mnist_model\"\n",
    ")\n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(mnist.train)[0]},\n",
    "    y=input(mnist.train)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(mnist.test)[0]},\n",
    "    y=input(mnist.test)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码来自于这里：\n",
    "\n",
    "https://codeburst.io/use-tensorflow-dnnclassifier-estimator-to-classify-mnist-dataset-a7222bf9f940\n",
    "Use Tensorflow DNNClassifier estimator to classify MNIST dataset\n",
    "（https://gist.githubusercontent.com/marcolanaro/67b77346730c0862b17c4800ee599286/raw/bb2dc2e8b4c962c78cf99b0b61e0d5adc6a53b64/mnist_estimator.py）\n",
    "\n",
    "\n",
    "它使用MNIST做为入口数据，使用2层DNN classifier来学习。它学习了10万步，精度不错。这里为了节省时间，仅学习了2000步。测试的精度可以达到：93.339998%\n",
    "\n",
    "从gnt-B.ipynb我们知道，入口数据就是3个对象：train, test, 及validation。然后它们分别有两个属性：images和labels。它们分别是numpy数组。\n",
    "\n",
    "我们将对gnt做同样的处理。\n",
    "\n",
    "对于学习的模型，原来的分类器是分了10类：\n",
    "\n",
    " n_classes=10,\n",
    " \n",
    " 我们需要分的就多了。有多少个需要识别的汉字，就需要分为多少类。\n",
    " 以test中的3755为目标吧。\n",
    " 即： n_classes=3755,\n",
    "\n",
    "下面尝试做gnt的入口数据处理：\n",
    "\n",
    "注意：要将RGB图变为灰度图：\n",
    "http://scikit-image.org/docs/dev/user_guide/transforming_image_data.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/sandbox/03755\n",
      "shape of images is:  (120, 64, 64)\n",
      "sandbox:  120 531 ../data/sandbox/00531/148941.png [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ..., \n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ..., \n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ..., \n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ..., \n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ..., \n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ..., \n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ..., \n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]] [[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "image_data:  [[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03c6d3a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import *\n",
    "from numpy import array\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "CHARSET_SIZE = 3755\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "class DataIterator:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set FLAGS.charset_size to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print(truncate_path)\n",
    "        self.image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                self.image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(self.image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in self.image_names]\n",
    "        self.images_rgb = [imread(file_name) for file_name in self.image_names]\n",
    "        self.image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in self.images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in self.image_resized]\n",
    "        self.images = array(self.images)\n",
    "        print ('shape of images is: ', self.images.shape)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "test_feeder = DataIterator(data_dir='../data/sandbox/')\n",
    "print ('sandbox: ', test_feeder.size, test_feeder.labels[0], test_feeder.image_names[0], test_feeder.images_rgb[0], test_feeder.images[0])\n",
    "\n",
    "\n",
    "# show image\n",
    "import matplotlib.pyplot as plt\n",
    "first_array=test_feeder.images[0]\n",
    "image_data = first_array.reshape((IMAGE_SIZE, IMAGE_SIZE))\n",
    "print ('image_data: ', image_data)\n",
    "\n",
    "\n",
    "#Not sure you even have to do that if you just want to visualize it\n",
    "#first_array=255*first_array\n",
    "#first_array=first_array.astype(\"uint8\")\n",
    "plt.imshow(image_data)\n",
    "#Actually displaying the plot if you are not in interactive mode\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面直接读出的，是3个数字（RGB？）一点的数组；如何得到如mnist，一个数一个点的数组？\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "train的00531目录下，确实是“呢”字。00054目录下，是“也”字。\n",
    "\n",
    "下面开始训练。由于train数据没有test多，即train只有部份字，而test有3755所有字。所以测试的精度肯定很低。不过，能跑起来，就可以了：\n",
    "\n",
    "注意，要删除杂质文件，如：.DS_Store\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: '._.DS_Store'\n",
    "\n",
    "huoyongxue@HSHY-151-SERVER:~/todo/gnt/data/train$ find . -name '.DS_Store' -type f -delete\n",
    "\n",
    "huoyongxue@HSHY-151-SERVER:~/todo/gnt/data$ rm train/._.DS_Store\n",
    "\n",
    "\n",
    "下面将仅训练及测试前37个字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_/00037\n",
      "../data/test_/00037\n",
      "Data loaded ...\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../dfs/checkpoint/dnn2_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "Begin to train ...\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn2_model/model.ckpt-22000\n",
      "INFO:tensorflow:Saving checkpoints for 22001 into ../dfs/checkpoint/dnn2_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 36.8806, step = 22001\n",
      "INFO:tensorflow:global_step/sec: 65.5926\n",
      "INFO:tensorflow:loss = 22.2147, step = 22101 (1.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3748\n",
      "INFO:tensorflow:loss = 32.6996, step = 22201 (1.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.4868\n",
      "INFO:tensorflow:loss = 46.3469, step = 22301 (1.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.1997\n",
      "INFO:tensorflow:loss = 24.2726, step = 22401 (1.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3976\n",
      "INFO:tensorflow:loss = 43.9817, step = 22501 (1.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.9122\n",
      "INFO:tensorflow:loss = 44.5547, step = 22601 (1.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.2455\n",
      "INFO:tensorflow:loss = 23.4116, step = 22701 (1.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3744\n",
      "INFO:tensorflow:loss = 34.4397, step = 22801 (1.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.9895\n",
      "INFO:tensorflow:loss = 34.815, step = 22901 (1.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.9832\n",
      "INFO:tensorflow:loss = 23.1699, step = 23001 (1.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.1757\n",
      "INFO:tensorflow:loss = 24.9514, step = 23101 (1.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3408\n",
      "INFO:tensorflow:loss = 32.7724, step = 23201 (1.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.1189\n",
      "INFO:tensorflow:loss = 28.6027, step = 23301 (1.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3643\n",
      "INFO:tensorflow:loss = 33.1075, step = 23401 (1.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8362\n",
      "INFO:tensorflow:loss = 40.4702, step = 23501 (1.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8911\n",
      "INFO:tensorflow:loss = 29.7753, step = 23601 (1.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.953\n",
      "INFO:tensorflow:loss = 19.1021, step = 23701 (1.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.6974\n",
      "INFO:tensorflow:loss = 24.1244, step = 23801 (1.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.8406\n",
      "INFO:tensorflow:loss = 31.0824, step = 23901 (1.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.7906\n",
      "INFO:tensorflow:loss = 25.9916, step = 24001 (1.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3571\n",
      "INFO:tensorflow:loss = 42.1585, step = 24101 (1.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.4558\n",
      "INFO:tensorflow:loss = 29.3565, step = 24201 (1.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.0306\n",
      "INFO:tensorflow:loss = 33.2468, step = 24301 (1.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.421\n",
      "INFO:tensorflow:loss = 22.3583, step = 24401 (1.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.6621\n",
      "INFO:tensorflow:loss = 25.8851, step = 24501 (1.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.2335\n",
      "INFO:tensorflow:loss = 27.6332, step = 24601 (1.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.3999\n",
      "INFO:tensorflow:loss = 24.9457, step = 24701 (1.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.4224\n",
      "INFO:tensorflow:loss = 25.4119, step = 24801 (1.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.2188\n",
      "INFO:tensorflow:loss = 23.5137, step = 24901 (1.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3455\n",
      "INFO:tensorflow:loss = 26.6446, step = 25001 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.639\n",
      "INFO:tensorflow:loss = 23.9046, step = 25101 (1.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1415\n",
      "INFO:tensorflow:loss = 25.2564, step = 25201 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2516\n",
      "INFO:tensorflow:loss = 21.4256, step = 25301 (1.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2668\n",
      "INFO:tensorflow:loss = 24.9097, step = 25401 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0488\n",
      "INFO:tensorflow:loss = 27.7099, step = 25501 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.9843\n",
      "INFO:tensorflow:loss = 15.2935, step = 25601 (1.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.9925\n",
      "INFO:tensorflow:loss = 25.3449, step = 25701 (1.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.6053\n",
      "INFO:tensorflow:loss = 35.1556, step = 25801 (1.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5748\n",
      "INFO:tensorflow:loss = 24.6425, step = 25901 (1.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.8492\n",
      "INFO:tensorflow:loss = 33.8692, step = 26001 (1.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3671\n",
      "INFO:tensorflow:loss = 20.628, step = 26101 (1.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1336\n",
      "INFO:tensorflow:loss = 33.2696, step = 26201 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1585\n",
      "INFO:tensorflow:loss = 33.9646, step = 26301 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2782\n",
      "INFO:tensorflow:loss = 27.877, step = 26401 (1.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3094\n",
      "INFO:tensorflow:loss = 22.6952, step = 26501 (1.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.7444\n",
      "INFO:tensorflow:loss = 24.2517, step = 26601 (1.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5766\n",
      "INFO:tensorflow:loss = 25.8521, step = 26701 (1.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.1924\n",
      "INFO:tensorflow:loss = 15.8609, step = 26801 (1.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5802\n",
      "INFO:tensorflow:loss = 22.578, step = 26901 (1.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.5481\n",
      "INFO:tensorflow:loss = 30.7397, step = 27001 (1.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5373\n",
      "INFO:tensorflow:loss = 30.6756, step = 27101 (1.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0226\n",
      "INFO:tensorflow:loss = 26.0349, step = 27201 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.1211\n",
      "INFO:tensorflow:loss = 21.0372, step = 27301 (1.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.2557\n",
      "INFO:tensorflow:loss = 28.7725, step = 27401 (1.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1577\n",
      "INFO:tensorflow:loss = 25.9612, step = 27501 (1.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.9092\n",
      "INFO:tensorflow:loss = 26.5778, step = 27601 (1.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1047\n",
      "INFO:tensorflow:loss = 21.2579, step = 27701 (1.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.0099\n",
      "INFO:tensorflow:loss = 28.4435, step = 27801 (1.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7832\n",
      "INFO:tensorflow:loss = 18.9105, step = 27901 (1.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7549\n",
      "INFO:tensorflow:loss = 23.1462, step = 28001 (1.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.4553\n",
      "INFO:tensorflow:loss = 20.2718, step = 28101 (1.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.8378\n",
      "INFO:tensorflow:loss = 17.3133, step = 28201 (1.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.9743\n",
      "INFO:tensorflow:loss = 26.3383, step = 28301 (1.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.2048\n",
      "INFO:tensorflow:loss = 14.9962, step = 28401 (1.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7372\n",
      "INFO:tensorflow:loss = 18.1465, step = 28501 (1.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2114\n",
      "INFO:tensorflow:loss = 28.1784, step = 28601 (1.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.2943\n",
      "INFO:tensorflow:loss = 20.2096, step = 28701 (1.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2614\n",
      "INFO:tensorflow:loss = 23.7203, step = 28801 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.8667\n",
      "INFO:tensorflow:loss = 19.8534, step = 28901 (1.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.871\n",
      "INFO:tensorflow:loss = 32.7229, step = 29001 (1.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.6058\n",
      "INFO:tensorflow:loss = 37.0748, step = 29101 (1.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7922\n",
      "INFO:tensorflow:loss = 20.4106, step = 29201 (1.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3615\n",
      "INFO:tensorflow:loss = 20.3362, step = 29301 (1.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.0033\n",
      "INFO:tensorflow:loss = 21.3333, step = 29401 (1.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5149\n",
      "INFO:tensorflow:loss = 18.0781, step = 29501 (1.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.3443\n",
      "INFO:tensorflow:loss = 14.8434, step = 29601 (1.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.7526\n",
      "INFO:tensorflow:loss = 21.7605, step = 29701 (1.394 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 71.1411\n",
      "INFO:tensorflow:loss = 32.1239, step = 29801 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.8178\n",
      "INFO:tensorflow:loss = 18.5574, step = 29901 (1.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.1452\n",
      "INFO:tensorflow:loss = 17.6657, step = 30001 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7656\n",
      "INFO:tensorflow:loss = 16.9358, step = 30101 (1.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.452\n",
      "INFO:tensorflow:loss = 14.8062, step = 30201 (1.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.602\n",
      "INFO:tensorflow:loss = 27.831, step = 30301 (1.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.009\n",
      "INFO:tensorflow:loss = 27.3041, step = 30401 (1.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3577\n",
      "INFO:tensorflow:loss = 31.3117, step = 30501 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.351\n",
      "INFO:tensorflow:loss = 14.7503, step = 30601 (1.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.4278\n",
      "INFO:tensorflow:loss = 22.7981, step = 30701 (1.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.3434\n",
      "INFO:tensorflow:loss = 23.6174, step = 30801 (1.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.6354\n",
      "INFO:tensorflow:loss = 17.8534, step = 30901 (1.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.2668\n",
      "INFO:tensorflow:loss = 13.7702, step = 31001 (1.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.6996\n",
      "INFO:tensorflow:loss = 27.7762, step = 31101 (1.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.3345\n",
      "INFO:tensorflow:loss = 15.3992, step = 31201 (1.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.4717\n",
      "INFO:tensorflow:loss = 30.6634, step = 31301 (1.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.8788\n",
      "INFO:tensorflow:loss = 25.0496, step = 31401 (1.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.6673\n",
      "INFO:tensorflow:loss = 23.7275, step = 31501 (1.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1073\n",
      "INFO:tensorflow:loss = 16.8121, step = 31601 (1.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2721\n",
      "INFO:tensorflow:loss = 11.761, step = 31701 (1.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6331\n",
      "INFO:tensorflow:loss = 19.2148, step = 31801 (1.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.8258\n",
      "INFO:tensorflow:loss = 28.6666, step = 31901 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.9363\n",
      "INFO:tensorflow:loss = 34.8885, step = 32001 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.1911\n",
      "INFO:tensorflow:loss = 18.9542, step = 32101 (1.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.9228\n",
      "INFO:tensorflow:loss = 22.1824, step = 32201 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.4711\n",
      "INFO:tensorflow:loss = 20.353, step = 32301 (1.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7437\n",
      "INFO:tensorflow:loss = 21.1262, step = 32401 (1.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.568\n",
      "INFO:tensorflow:loss = 23.8695, step = 32501 (1.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.9862\n",
      "INFO:tensorflow:loss = 26.4527, step = 32601 (1.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.8512\n",
      "INFO:tensorflow:loss = 19.4096, step = 32701 (1.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.8932\n",
      "INFO:tensorflow:loss = 19.4862, step = 32801 (1.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.2078\n",
      "INFO:tensorflow:loss = 18.7238, step = 32901 (1.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2174\n",
      "INFO:tensorflow:loss = 36.44, step = 33001 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1904\n",
      "INFO:tensorflow:loss = 12.6718, step = 33101 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7976\n",
      "INFO:tensorflow:loss = 25.9231, step = 33201 (1.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.331\n",
      "INFO:tensorflow:loss = 18.2974, step = 33301 (1.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7264\n",
      "INFO:tensorflow:loss = 17.2962, step = 33401 (1.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.9825\n",
      "INFO:tensorflow:loss = 21.4313, step = 33501 (1.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.1888\n",
      "INFO:tensorflow:loss = 20.6333, step = 33601 (1.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3263\n",
      "INFO:tensorflow:loss = 22.3407, step = 33701 (1.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.3487\n",
      "INFO:tensorflow:loss = 17.6685, step = 33801 (1.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.5747\n",
      "INFO:tensorflow:loss = 21.5424, step = 33901 (1.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3717\n",
      "INFO:tensorflow:loss = 13.6218, step = 34001 (1.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.4155\n",
      "INFO:tensorflow:loss = 22.1409, step = 34101 (1.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.6463\n",
      "INFO:tensorflow:loss = 17.5172, step = 34201 (1.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.0709\n",
      "INFO:tensorflow:loss = 15.3796, step = 34301 (1.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.064\n",
      "INFO:tensorflow:loss = 26.917, step = 34401 (1.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.9432\n",
      "INFO:tensorflow:loss = 26.3835, step = 34501 (1.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6342\n",
      "INFO:tensorflow:loss = 23.7518, step = 34601 (1.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.9898\n",
      "INFO:tensorflow:loss = 19.787, step = 34701 (1.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.0004\n",
      "INFO:tensorflow:loss = 25.7684, step = 34801 (1.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5239\n",
      "INFO:tensorflow:loss = 18.0113, step = 34901 (1.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7062\n",
      "INFO:tensorflow:loss = 34.4725, step = 35001 (1.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8886\n",
      "INFO:tensorflow:loss = 15.7866, step = 35101 (1.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.1957\n",
      "INFO:tensorflow:loss = 24.8234, step = 35201 (1.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.4305\n",
      "INFO:tensorflow:loss = 19.3325, step = 35301 (1.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7276\n",
      "INFO:tensorflow:loss = 19.2239, step = 35401 (1.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3192\n",
      "INFO:tensorflow:loss = 16.8548, step = 35501 (1.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.2654\n",
      "INFO:tensorflow:loss = 13.856, step = 35601 (1.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.8686\n",
      "INFO:tensorflow:loss = 13.9442, step = 35701 (1.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.0652\n",
      "INFO:tensorflow:loss = 19.9194, step = 35801 (1.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.1436\n",
      "INFO:tensorflow:loss = 12.4639, step = 35901 (1.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.2186\n",
      "INFO:tensorflow:loss = 13.7005, step = 36001 (1.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.868\n",
      "INFO:tensorflow:loss = 18.7232, step = 36101 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3277\n",
      "INFO:tensorflow:loss = 21.3759, step = 36201 (1.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.7245\n",
      "INFO:tensorflow:loss = 16.5089, step = 36301 (1.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.2413\n",
      "INFO:tensorflow:loss = 18.0903, step = 36401 (1.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3582\n",
      "INFO:tensorflow:loss = 23.5541, step = 36501 (1.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.1819\n",
      "INFO:tensorflow:loss = 17.9397, step = 36601 (1.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.9673\n",
      "INFO:tensorflow:loss = 21.5785, step = 36701 (1.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.2409\n",
      "INFO:tensorflow:loss = 16.0428, step = 36801 (1.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.7618\n",
      "INFO:tensorflow:loss = 16.6797, step = 36901 (1.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.7512\n",
      "INFO:tensorflow:loss = 16.8815, step = 37001 (1.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5867\n",
      "INFO:tensorflow:loss = 12.7601, step = 37101 (1.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5409\n",
      "INFO:tensorflow:loss = 13.1356, step = 37201 (1.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.5445\n",
      "INFO:tensorflow:loss = 16.2356, step = 37301 (1.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.5724\n",
      "INFO:tensorflow:loss = 32.2006, step = 37401 (1.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.4965\n",
      "INFO:tensorflow:loss = 15.3534, step = 37501 (1.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5478\n",
      "INFO:tensorflow:loss = 11.8717, step = 37601 (1.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.2272\n",
      "INFO:tensorflow:loss = 16.1143, step = 37701 (1.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.4682\n",
      "INFO:tensorflow:loss = 19.5373, step = 37801 (1.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7604\n",
      "INFO:tensorflow:loss = 19.219, step = 37901 (1.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7804\n",
      "INFO:tensorflow:loss = 11.8622, step = 38001 (1.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.7781\n",
      "INFO:tensorflow:loss = 13.9818, step = 38101 (1.475 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 68.9259\n",
      "INFO:tensorflow:loss = 22.5674, step = 38201 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.2563\n",
      "INFO:tensorflow:loss = 18.7244, step = 38301 (1.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.1149\n",
      "INFO:tensorflow:loss = 13.5106, step = 38401 (1.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3359\n",
      "INFO:tensorflow:loss = 10.3801, step = 38501 (1.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.1444\n",
      "INFO:tensorflow:loss = 13.6965, step = 38601 (1.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.9029\n",
      "INFO:tensorflow:loss = 11.3877, step = 38701 (1.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3495\n",
      "INFO:tensorflow:loss = 19.6706, step = 38801 (1.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.687\n",
      "INFO:tensorflow:loss = 20.5509, step = 38901 (1.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.8361\n",
      "INFO:tensorflow:loss = 17.8339, step = 39001 (1.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.3487\n",
      "INFO:tensorflow:loss = 17.2506, step = 39101 (1.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6321\n",
      "INFO:tensorflow:loss = 16.9592, step = 39201 (1.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.1379\n",
      "INFO:tensorflow:loss = 15.0948, step = 39301 (1.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8362\n",
      "INFO:tensorflow:loss = 12.5055, step = 39401 (1.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.4642\n",
      "INFO:tensorflow:loss = 16.5325, step = 39501 (1.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.0014\n",
      "INFO:tensorflow:loss = 12.4281, step = 39601 (1.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7272\n",
      "INFO:tensorflow:loss = 17.0244, step = 39701 (1.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.9212\n",
      "INFO:tensorflow:loss = 9.72262, step = 39801 (1.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.1515\n",
      "INFO:tensorflow:loss = 21.3174, step = 39901 (1.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6779\n",
      "INFO:tensorflow:loss = 13.6749, step = 40001 (1.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6846\n",
      "INFO:tensorflow:loss = 13.5976, step = 40101 (1.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.6562\n",
      "INFO:tensorflow:loss = 24.7449, step = 40201 (1.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.7112\n",
      "INFO:tensorflow:loss = 13.8448, step = 40301 (1.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7055\n",
      "INFO:tensorflow:loss = 7.42068, step = 40401 (1.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.0205\n",
      "INFO:tensorflow:loss = 13.2091, step = 40501 (1.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3972\n",
      "INFO:tensorflow:loss = 13.7354, step = 40601 (1.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.7049\n",
      "INFO:tensorflow:loss = 16.1049, step = 40701 (1.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.2669\n",
      "INFO:tensorflow:loss = 14.2327, step = 40801 (1.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.2595\n",
      "INFO:tensorflow:loss = 19.8469, step = 40901 (1.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.0075\n",
      "INFO:tensorflow:loss = 18.0585, step = 41001 (1.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.0227\n",
      "INFO:tensorflow:loss = 13.8162, step = 41101 (1.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3936\n",
      "INFO:tensorflow:loss = 21.7493, step = 41201 (1.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6576\n",
      "INFO:tensorflow:loss = 19.5426, step = 41301 (1.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6941\n",
      "INFO:tensorflow:loss = 24.6753, step = 41401 (1.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.4181\n",
      "INFO:tensorflow:loss = 26.1464, step = 41501 (1.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3465\n",
      "INFO:tensorflow:loss = 9.78494, step = 41601 (1.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5683\n",
      "INFO:tensorflow:loss = 13.9244, step = 41701 (1.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.8946\n",
      "INFO:tensorflow:loss = 17.0078, step = 41801 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.299\n",
      "INFO:tensorflow:loss = 13.8482, step = 41901 (1.486 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 42000 into ../dfs/checkpoint/dnn2_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 15.4139.\n",
      "Train done ...\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-19-01:21:08\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn2_model/model.ckpt-42000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-19-01:21:09\n",
      "INFO:tensorflow:Saving dict for global step 42000: accuracy = 0.988688, average_loss = 0.0892466, global_step = 42000, loss = 10.9575\n",
      "\n",
      "Test Accuracy: 98.868775%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set FLAGS.charset_size to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        \n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "    \n",
    "train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "print ('Data loaded ...')\n",
    "\n",
    "\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "# Build 2 layer DNN classifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[256, 32],\n",
    "    optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "    n_classes=CHARSET_SIZE,\n",
    "    dropout=0.1,\n",
    "    model_dir=\"../dfs/checkpoint/dnn2_model\"\n",
    ")\n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(train_data)[0]},\n",
    "    y=input(train_data)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")\n",
    "print ('Begin to train ...')\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=20000)\n",
    "print ('Train done ...')\n",
    "\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用5层DNN进行训练：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing path:  ../data/train_/00037\n",
      "Train data loaded.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../dfs/checkpoint/dnn5_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn5_model/model.ckpt-20001\n",
      "INFO:tensorflow:Saving checkpoints for 20002 into ../dfs/checkpoint/dnn5_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 30.754, step = 20002\n",
      "INFO:tensorflow:global_step/sec: 4.72568\n",
      "INFO:tensorflow:loss = 33.6694, step = 20102 (21.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.37136\n",
      "INFO:tensorflow:loss = 25.8949, step = 20202 (18.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24306\n",
      "INFO:tensorflow:loss = 18.7259, step = 20302 (16.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.79179\n",
      "INFO:tensorflow:loss = 30.5855, step = 20402 (14.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.1549\n",
      "INFO:tensorflow:loss = 27.091, step = 20502 (13.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.96911\n",
      "INFO:tensorflow:loss = 28.019, step = 20602 (14.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.02681\n",
      "INFO:tensorflow:loss = 32.0103, step = 20702 (14.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.43518\n",
      "INFO:tensorflow:loss = 33.2493, step = 20802 (13.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.30973\n",
      "INFO:tensorflow:loss = 18.9766, step = 20902 (15.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.28306\n",
      "INFO:tensorflow:loss = 27.9294, step = 21002 (18.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.01653\n",
      "INFO:tensorflow:loss = 15.9792, step = 21102 (19.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.96274\n",
      "INFO:tensorflow:loss = 14.9548, step = 21202 (20.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97667\n",
      "INFO:tensorflow:loss = 29.9858, step = 21302 (20.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97479\n",
      "INFO:tensorflow:loss = 12.9294, step = 21402 (20.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97291\n",
      "INFO:tensorflow:loss = 17.2585, step = 21502 (20.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.98118\n",
      "INFO:tensorflow:loss = 7.75592, step = 21602 (20.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.94694\n",
      "INFO:tensorflow:loss = 24.2837, step = 21702 (20.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.93546\n",
      "INFO:tensorflow:loss = 17.9006, step = 21802 (20.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.03259\n",
      "INFO:tensorflow:loss = 27.9533, step = 21902 (19.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.98391\n",
      "INFO:tensorflow:loss = 17.88, step = 22002 (20.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97478\n",
      "INFO:tensorflow:loss = 35.9466, step = 22102 (20.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.99693\n",
      "INFO:tensorflow:loss = 20.8485, step = 22202 (20.013 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.97536\n",
      "INFO:tensorflow:loss = 9.07572, step = 22302 (20.101 sec)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set CHARSET_SIZE to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        \n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "    \n",
    "train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "print ('Train data loaded.')\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "# Build 2 layer DNN classifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[4096, 256, 256, 256, 64],\n",
    "    optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "    n_classes=CHARSET_SIZE,\n",
    "    dropout=0.1,\n",
    "    model_dir=\"../dfs/checkpoint/dnn5_model\"\n",
    ")\n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(train_data)[0]},\n",
    "    y=input(train_data)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, steps=200000)\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "print ('Train done, begin to test ...')\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上次的model，继续train：参考https://colab.research.google.com/drive/1eSVbFPcHHt1BbSKwzdclRDeLIIx_5ZOR\n",
    "linear-regression-D.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing path:  ../data/train_/00037\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa766334f98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../dfs/checkpoint/dnn5_model'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn5_model/model.ckpt-23201\n",
      "INFO:tensorflow:Saving checkpoints for 23202 into ../dfs/checkpoint/dnn5_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 7.43387, step = 23202\n",
      "INFO:tensorflow:global_step/sec: 7.65526\n",
      "INFO:tensorflow:loss = 7.92822, step = 23302 (13.066 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23401 into ../dfs/checkpoint/dnn5_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 10.0225.\n",
      "Train done, begin to test ...\n",
      "Now processing path:  ../data/test_/00037\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-19-01:43:00\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn5_model/model.ckpt-23401\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-19-01:43:01\n",
      "INFO:tensorflow:Saving dict for global step 23401: accuracy = 0.98552, average_loss = 0.0478588, global_step = 23401, loss = 5.87599\n",
      "\n",
      "Test Accuracy: 98.552036%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from: https://www.kaggle.com/jeffcarp/example-save-and-load-a-tensorflow-model\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.estimators import run_config\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "# Define the training inputs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(train_data)[0]},\n",
    "    y=input(train_data)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set CHARSET_SIZE to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        \n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "    \n",
    "train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "def make_estimator(model_dir):\n",
    "    config = run_config.RunConfig(model_dir=model_dir)\n",
    "\n",
    "    return tf.estimator.DNNClassifier (\n",
    "        config=config,\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[4096, 256, 256, 256, 64],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=CHARSET_SIZE,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "MODEL_DIR = \"../dfs/checkpoint/dnn5_model\"\n",
    "model_from_checkpoint = make_estimator(MODEL_DIR)\n",
    "\n",
    "model_from_checkpoint.train(input_fn=train_input_fn, steps=200)\n",
    "print ('Train done, begin to test ...')\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = model_from_checkpoint.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面还真好使：\n",
    "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn5_model/model.ckpt-20000\n",
    "INFO:tensorflow:Saving checkpoints for 20001 into ../dfs/checkpoint/dnn5_model/model.ckpt.\n",
    "INFO:tensorflow:loss = 5.79399, step = 20001\n",
    "\n",
    "从上次的20000步开始。从这里看来，即前使用浏览器训练时没有了响应，后台也是保存了的。就是前台失去了联系。\n",
    "下面我们做点有趣的：手工观察测试情况。参考https://colab.research.google.com/drive/1eSVbFPcHHt1BbSKwzdclRDeLIIx_5ZOR#scrollTo=JkbP89CJd1bC （linear-regression-D.ipynb）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febdc9167b8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../dfs/checkpoint/dnn5_model'}\n",
      "Now processing path:  ../data/test_/00037\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-21-02:14:28\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn5_model/model.ckpt-220001\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-21-02:14:30\n",
      "INFO:tensorflow:Saving dict for global step 220001: accuracy = 0.823077, average_loss = 1.31783, global_step = 220001, loss = 161.8\n",
      "\n",
      "Test Accuracy: 82.307690%\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ../dfs/checkpoint/dnn5_model/model.ckpt-220001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb70b0c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  26\n",
      "Real label is: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb708d7208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  35\n",
      "Real label is: 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb587806d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  21\n",
      "Real label is: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb587705f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  35\n",
      "Real label is: 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb586de518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  26\n",
      "Real label is: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb5864e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  31\n",
      "Real label is: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb587366d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  11\n",
      "Real label is: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb724de048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  25\n",
      "Real label is: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb805b3630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  28\n",
      "Real label is: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb805b2d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  10\n",
      "Real label is: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb6c634b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  21\n",
      "Real label is: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb586bfa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  9\n",
      "Real label is: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb58741898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  25\n",
      "Real label is: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb5872c710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  21\n",
      "Real label is: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb407e1438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  6\n",
      "Real label is: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb724fa2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  20\n",
      "Real label is: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb723f3198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  1\n",
      "Real label is: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb725c0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  36\n",
      "Real label is: 36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb72581e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  9\n",
      "Real label is: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb725d16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  1\n",
      "Real label is: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb7249ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  27\n",
      "Real label is: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb724deef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  17\n",
      "Real label is: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb725dfe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  28\n",
      "Real label is: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb707e0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  1\n",
      "Real label is: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb72793ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  35\n",
      "Real label is: 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb72478908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  33\n",
      "Real label is: 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb724eb828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  13\n",
      "Real label is: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb72622668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  28\n",
      "Real label is: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb587dc390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  15\n",
      "Real label is: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb708742e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  34\n",
      "Real label is: 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb7238a128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  13\n",
      "Real label is: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb587709b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  2\n",
      "Real label is: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb7260e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  29\n",
      "Real label is: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb6c63f5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  32\n",
      "Real label is: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb76d46550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  14\n",
      "Real label is: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb805a4ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  32\n",
      "Real label is: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb6c628c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label is:  29\n",
      "Real label is: 29\n"
     ]
    }
   ],
   "source": [
    "# from: https://www.kaggle.com/jeffcarp/example-save-and-load-a-tensorflow-model\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.estimators import run_config\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set CHARSET_SIZE to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        \n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "def make_estimator(model_dir):\n",
    "    config = run_config.RunConfig(model_dir=model_dir)\n",
    "\n",
    "    return tf.estimator.DNNClassifier (\n",
    "        config=config,\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[4096, 256, 256, 256, 64],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=CHARSET_SIZE,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "MODEL_DIR = \"../dfs/checkpoint/dnn5_model\"\n",
    "model_from_checkpoint = make_estimator(MODEL_DIR)\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = model_from_checkpoint.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "\n",
    "# manually test\n",
    "predictions = list(model_from_checkpoint.predict(input_fn=predict_input_fn))\n",
    "predictions_value_array = list(predictions)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_n_hanzi(predictions_values, index):\n",
    "    # Show the image of the input data\n",
    "    image_data_array = test_data.images[index]\n",
    "    image_data = image_data_array.reshape((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    plt.imshow(image_data)\n",
    "    plt.show()\n",
    "    \n",
    "    # show the prediction result\n",
    "    print ('Prediction label is: ', np.argmax(predictions_values[index][\"probabilities\"]))\n",
    "    \n",
    "    # Show the lable\n",
    "    print (\"Real label is: %d\"%(test_data.labels[index]))\n",
    "\n",
    "for i in range(37):\n",
    "    test_n_hanzi(predictions_value_array, i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
