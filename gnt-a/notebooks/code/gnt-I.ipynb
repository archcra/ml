{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们基于gnt-H的10层模型，应用上左右随机30度以内的旋转，再加上gamma随机校正的训练数据，进行训练。以观察精度增长情况。\n",
    "\n",
    "这个应该很耗时。但是，希望在4万步时，精度能超过83%。（8层在4万步时，精度达到了84%）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage.exposure import adjust_gamma\n",
    "from skimage.transform import rotate\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "FULL_CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def getRndGamma():\n",
    "    rnd1 = np.random.random_sample()\n",
    "    rnd2 = np.random.random_sample()+0.001\n",
    "    rnd3 = np.random.random_sample()*100\n",
    "\n",
    "    if (rnd1 > 0.5):\n",
    "        return rnd3\n",
    "    else:\n",
    "        return rnd2\n",
    "    \n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set CHARSET_SIZE to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        \n",
    "        # add random rotate\n",
    "        image_rotated = [rotate(image, np.random.random_sample()*60-30) for image in image_resized]\n",
    "\n",
    "        # add random gamma adjust\n",
    "        image_gamma_adjusted = [adjust_gamma(image, getRndGamma()) for image in image_rotated]\n",
    "\n",
    "        self.images = [rgb2gray(item) for item in image_gamma_adjusted]\n",
    "        \n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "# Build 8 layer DNN classifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns, # The input features to our model\n",
    "    hidden_units=[4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096], # 8 hidden layers\n",
    "    n_classes=CHARSET_SIZE, # survived or not {1, 0}\n",
    "    model_dir=\"../dfs/checkpoint/dnn10_model_b\", # Path to where checkpoints etc are stored\n",
    "    optimizer=tf.train.RMSPropOptimizer(\n",
    "        learning_rate=0.00001),\n",
    "    dropout=0.1)\n",
    "\n",
    "for i in range(5):\n",
    "    train_data = DataSetLoader(data_dir='../data/train_/')\n",
    "    print ('Train data loaded for the %d times.'%(i+1))\n",
    "    # Define the training inputs\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": input(train_data)[0]},\n",
    "        y=input(train_data)[1],\n",
    "        num_epochs=None,\n",
    "        batch_size=50,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    classifier.train(input_fn=train_input_fn, steps=10000)\n",
    "\n",
    "print ('Train done, begin to test ...')\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是单独测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://www.kaggle.com/jeffcarp/example-save-and-load-a-tensorflow-model\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.estimators import run_config\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "# CHARSET_SIZE = 3755\n",
    "CHARSET_SIZE = 37\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels\n",
    "\n",
    "class DataSetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        # Set CHARSET_SIZE to a small value if available computation power is limited.\n",
    "        truncate_path = data_dir + ('%05d' % CHARSET_SIZE)\n",
    "        print('Now processing path: ', truncate_path)\n",
    "        image_names = []\n",
    "        for root, sub_folder, file_list in os.walk(data_dir):\n",
    "            if root < truncate_path:\n",
    "                image_names += [os.path.join(root, file_path) for file_path in file_list]\n",
    "        random.shuffle(image_names)\n",
    "        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in image_names]\n",
    "        images_rgb = [imread(file_name) for file_name in image_names]\n",
    "        image_resized = [resize(image, (IMAGE_SIZE, IMAGE_SIZE)) for image in images_rgb]\n",
    "        self.images = [rgb2gray(item) for item in image_resized]\n",
    "        \n",
    "        # convert list to numpy array\n",
    "        self.images = array(self.images)\n",
    "        self.labels = array(self.labels)\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[IMAGE_SIZE, IMAGE_SIZE])]\n",
    "\n",
    "def make_estimator(model_dir):\n",
    "    config = run_config.RunConfig(model_dir=model_dir)\n",
    "\n",
    "    return tf.estimator.DNNClassifier (\n",
    "        config=config,\n",
    "        feature_columns=feature_columns, # The input features to our model\n",
    "    hidden_units=[4096, 4096, 4096, 4096, 4096, 4096, 4096, 4096], # 8 hidden layers\n",
    "    n_classes=CHARSET_SIZE, # survived or not {1, 0}\n",
    "    optimizer=tf.train.RMSPropOptimizer(\n",
    "        learning_rate=0.00001),\n",
    "    dropout=0.1)\n",
    "    \n",
    "\n",
    "MODEL_DIR = \"../dfs/checkpoint/dnn10_model_b\"\n",
    "model_from_checkpoint = make_estimator(MODEL_DIR)\n",
    "\n",
    "test_data = DataSetLoader(data_dir='../data/test_/')\n",
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    y=input(test_data)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(test_data)[0]},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_score = model_from_checkpoint.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
